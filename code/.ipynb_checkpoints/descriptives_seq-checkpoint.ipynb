{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c662f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d9994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/sequence-level/data_seq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27865054",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = pd.read_csv('../../data/sequence-level/data_seq_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54664ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8526181081369308"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of sequences of minimum length 20\n",
    "data_final.groupby('sentenceID').filter(lambda x: len(x) >= 20).sentenceID.nunique()/data_final.sentenceID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b16f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_item_support = 5\n",
    "min_session_length = 2\n",
    "min_user_sessions = 3\n",
    "drop_first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "628255e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_KEY = 'userID'\n",
    "TIME_KEY = 'timestamp'\n",
    "ITEM_KEY = 'usID'\n",
    "SESSION_KEY = 'sentenceID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "022cc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "data = data[data['seq_freq']>=N].groupby(['sessionID']).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2b86ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_day = pd.to_datetime(data.timestamp.min(), unit='s').date()\n",
    "end_day = pd.to_datetime(data.timestamp.max(), unit='s').date()\n",
    "day_range = pd.date_range(start_day, end_day, freq='D')\n",
    "\n",
    "# helper list (same length as data) containing the day\n",
    "user_day = data['userID'].astype(str).str.zfill(3) + '_' + pd.to_datetime(data['timestamp'], unit='s').apply(lambda x: x.date()).astype(str)\n",
    "\n",
    "sentence_mapping = dict([(y,x+1) for x,y in enumerate(sorted(set(user_day)))])\n",
    "sentence_indexes = [sentence_mapping[x] for x in user_day]\n",
    "\n",
    "data['sentenceID'] = sentence_indexes\n",
    "data['sentence_freq'] = data.groupby([SESSION_KEY])[SESSION_KEY].transform('size')\n",
    "\n",
    "session_lengths = data.groupby('sentenceID').sentenceID.count()\n",
    "q1_eps = np.quantile(session_lengths.values, 0.25)\n",
    "median_eps = np.quantile(session_lengths.values, 0.50)\n",
    "q3_eps = np.quantile(session_lengths.values, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba413d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sequence-level analysis, user sequences are tokens (events) and daily concatenations thereof are sentences (sessions).\n",
      "For sequence-level analysis, only tokens with frequency of at least 10 were included\n",
      "The sequence-level data contains a total of:\n",
      "     - 9377 sentences\n",
      "     - 234 sentences with only one token (excluded during data split due to minimum session length of 2)\n",
      "     - 720379 tokens\n",
      "     - 2454 unique tokens\n",
      "1st quartile of events per session: 34.0\n",
      "median number of events per session: 66.0\n",
      "3rd quartile of events per session: 106.0\n"
     ]
    }
   ],
   "source": [
    "print('For sequence-level analysis, user sequences are tokens (events) and daily concatenations thereof are sentences (sessions).')\n",
    "print('For sequence-level analysis, only tokens with frequency of at least ' + str(N) + ' were included')\n",
    "print('The sequence-level data contains a total of:')\n",
    "print('     - ' + str(data[SESSION_KEY].nunique()) + ' sentences')\n",
    "print('     - ' + str(sum(data.sentence_freq==1)) + ' sentences with only one token (excluded during data split due to minimum session length of 2)')\n",
    "print('     - ' + str(data.shape[0]) + ' tokens')\n",
    "print('     - ' + str(data.usID.nunique()) + ' unique tokens')\n",
    "print('1st quartile of events per session: ' + str(round(q1_eps, 2)))\n",
    "print('median number of events per session: ' + str(round(median_eps, 2)))\n",
    "print('3rd quartile of events per session: ' + str(round(q3_eps, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3699cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentenceID\n",
       "1        60\n",
       "2        57\n",
       "3        12\n",
       "4        15\n",
       "5        54\n",
       "       ... \n",
       "9373     12\n",
       "9374      2\n",
       "9375     62\n",
       "9376    105\n",
       "9377     57\n",
       "Name: usID, Length: 9377, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('sentenceID').sentenceID.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "123e454b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sequence-level analysis, we have: \n",
      "     - 720379 sentences of length >= 1\n",
      "     - 720145 sentences of length >= 2\n",
      "     - 719851 sentences of length >= 3\n",
      "     - 719608 sentences of length >= 4\n",
      "     - 719376 sentences of length >= 5\n",
      "     - 719076 sentences of length >= 6\n",
      "     - 718692 sentences of length >= 7\n",
      "     - 718342 sentences of length >= 8\n",
      "     - 717894 sentences of length >= 9\n",
      "     - 717498 sentences of length >= 10\n",
      "     - 713946 sentences of length >= 15\n",
      "     - 708961 sentences of length >= 20\n",
      "     - 634510 sentences of length >= 50\n",
      "     - 397801 sentences of length >= 100\n",
      "     - 93476 sentences of length >= 200\n",
      "     - 3830 sentences of length >= 500\n",
      "Clearly, most of the sentences are not short. Therefore, restricting ourselves to sentences of a certain minimum length, say, 20, would not have much of an impact.\n",
      "Based on this, language modeling techniques should work reasonably well.\n",
      "Furthermore, we might expect LM techniques to perform rather well on predicting tokens in higher positions.\n"
     ]
    }
   ],
   "source": [
    "print('For sequence-level analysis, we have: ')\n",
    "for i in [1,2,3,4,5,6,7,8,9,10,15,20,50,100,200,500]:\n",
    "    print('     - ' + str(sum(data.sentence_freq>=i)) + ' sentences of length >= ' + str(i))\n",
    "print('Clearly, most of the sentences are not short. Therefore, restricting ourselves to sentences of a certain minimum length, say, 20, would not have much of an impact.')\n",
    "print('Based on this, language modeling techniques should work reasonably well.')\n",
    "print('Furthermore, we might expect LM techniques to perform rather well on predicting tokens in higher positions.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
