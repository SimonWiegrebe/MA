{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dataset 1 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pyreadr.read_r('../../data/sequences_all_anon.Rds')[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.rename(columns={'datum':'date', 'value':'category', 'anon_apps.name':'app_name'}, inplace=True)\n",
    "data_raw['timestamp'] = data_raw['date'].apply(lambda x: x.timestamp())\n",
    "data_raw.loc[data_raw['app_name'].isnull(),'app_name'] = data_raw['category'] # replace NaNs in app_name by corresponding category value\n",
    "data_raw['sessionID'] = data_raw['app_name'].shift(1).isin(['OFF_LOCKED','OFF_UNLOCKED']).cumsum() + 1 # sessionID is like sequence_number but does NOT start anew for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.to_csv('../../data/data_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### datasets 1, 2 and 3 combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_raw_1 = pyreadr.read_r('../../data/sequences_all_anon.Rds')[None]\n",
    "# data_raw_2 = pyreadr.read_r('../../data/sequences_all_anon_1.Rds')[None]\n",
    "# data_raw_3 = pyreadr.read_r('../../data/sequences_all_anon_wild.Rds')[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(data_raw_1))\n",
    "# print(len(data_raw_2))\n",
    "# print(len(data_raw_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(data_raw_1)/(len(data_raw_1)+len(data_raw_2)+len(data_raw_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_raw = pd.concat([data_raw_1, data_raw_2, data_raw_3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_raw_1.datum.min())\n",
    "# print(data_raw_1.datum.max())\n",
    "# print(data_raw_2.datum.min())\n",
    "# print(data_raw_2.datum.max())\n",
    "# print(data_raw_3.datum.min())\n",
    "# print(data_raw_3.datum.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### session-aware data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sa = pd.read_csv('../../data/data_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_mapping = dict([(y,x+1) for x,y in enumerate(sorted(set(data_sa['app_name'])))])\n",
    "app_indexes = [app_mapping[x] for x in data_sa['app_name']]\n",
    "# print(len(set(app_indexes)) == data_sa['app_name'].nunique()) # check\n",
    "\n",
    "user_mapping = dict([(y,x+1) for x,y in enumerate(sorted(set(data_sa['userId'])))])\n",
    "user_indexes = [user_mapping[x] for x in data_sa['userId']]\n",
    "# print(len(set(user_indexes)) == data_sa['userId'].nunique()) # check\n",
    "\n",
    "data_sa['appID'] = app_indexes\n",
    "data_sa.insert(0, 'userID', user_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_mapping_reverse = dict((v,k) for k,v in app_mapping.items())\n",
    "user_mapping_reverse = dict((v,k) for k,v in user_mapping.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messaging_1\n"
     ]
    }
   ],
   "source": [
    "print(list(app_mapping.keys())[list(app_mapping.values()).index(1194)])\n",
    "print(app_mapping_reverse[1194])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sa = data_sa.drop(['userId', 'date', 'activity', 'category', 'sequence_number', 'app_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sa.to_csv('../../data/data_sa.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-validation-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sa = pd.read_csv('../../data/data_sa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_KEY = 'userID'\n",
    "ITEM_KEY = 'appID'\n",
    "TIME_KEY = 'timestamp'\n",
    "SESSION_KEY = 'sessionID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, min_item_support=5, min_session_length=2, min_user_sessions=3,\n",
    "               drop_on=False, drop_off=False, drop_first=False):\n",
    "    '''\n",
    "    Preprocesses the dataframe by filtering out infrequent items, short sessions, and users with few sessions\n",
    "    -----\n",
    "        df: Pandas dataframe\n",
    "            Must contain the following columns: USER_KEY; ITEM_KEY; TIME_KEY; SESSION_KEY\n",
    "        drop_first: boolean\n",
    "            whether the first item of each session should be dropped\n",
    "        min_item_support: integer\n",
    "            minimum number of occurrences of an item (app) across all users and sessions for an item to be included\n",
    "        min_session_length: integer\n",
    "            minimum length (number of items) of a session for a session to be included\n",
    "        min_user_sessions: integer\n",
    "            minimum number of sessions per user for a user to be included\n",
    "    '''\n",
    "    if drop_first:\n",
    "        mask = df[ITEM_KEY].shift(-1).isin([1389, 1390]) # 1389=\"OFF_LOCKED\", 1390=\"OFF_UNLOCKED\"\n",
    "        df = df[~mask] # filter out the first item of each session, i.e., items PRECEDED by 1389 or 1390\n",
    "    if drop_on:\n",
    "        mask = df[ITEM_KEY].isin([1392, 1393])\n",
    "        df = df[~mask]\n",
    "    if drop_off:\n",
    "        mask = df[ITEM_KEY].isin([1389, 1390])\n",
    "        df = df[~mask]\n",
    "    # min_item_support\n",
    "    df = df.groupby(ITEM_KEY).filter(lambda x: len(x) >= min_item_support)\n",
    "    # min_session_length\n",
    "    if df.groupby(SESSION_KEY)[SESSION_KEY].size().min() < min_session_length:\n",
    "        df = df.groupby(SESSION_KEY).filter(lambda x: len(x) >= min_session_length)\n",
    "    # min_user_sessions\n",
    "    user_sessions = df.groupby([USER_KEY])[SESSION_KEY].nunique()\n",
    "    mask = df[USER_KEY].apply(lambda x: user_sessions[x]) >= min_user_sessions\n",
    "    df = df[mask]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_last_session(df):\n",
    "    '''\n",
    "    Splits off the last session of a sequence of sessions for each user\n",
    "    -----\n",
    "        df: Pandas dataframe\n",
    "            Must contain the following columns: USER_KEY; ITEM_KEY; TIME_KEY; SESSION_KEY\n",
    "    '''\n",
    "    last_sessions = df[SESSION_KEY].groupby(df[USER_KEY]).transform('last')\n",
    "    train = df[df[SESSION_KEY]!=last_sessions]\n",
    "    test = df[df[SESSION_KEY]==last_sessions]\n",
    "    \n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_new_items(train, test):\n",
    "    '''\n",
    "    Filters out observations from a test set which do not appear in the corresponding training set\n",
    "    -----\n",
    "        train: Pandas dataframe\n",
    "            Training set; must contain the following columns: USER_KEY; ITEM_KEY; TIME_KEY; SESSION_KEY\n",
    "        test: Pandas dataframe\n",
    "            Test set; must contain the following columns: USER_KEY; ITEM_KEY; TIME_KEY; SESSION_KEY\n",
    "    '''\n",
    "    test = test[test[ITEM_KEY].isin(train[ITEM_KEY].unique())]\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df,\n",
    "               min_item_support, min_session_length, min_user_sessions,\n",
    "               USER_KEY, ITEM_KEY, TIME_KEY, SESSION_KEY,\n",
    "               drop_on=False, drop_off=False, drop_first=False):\n",
    "    df_preprocessed = preprocess(df, min_item_support=5, min_session_length=2, min_user_sessions=3,\n",
    "                                 drop_on=drop_on, drop_off=drop_off, drop_first=drop_first)\n",
    "    train, test = split_last_session(df_preprocessed)\n",
    "    valid_train, valid_test = split_last_session(train)\n",
    "    test = filter_new_items(train, test)\n",
    "    valid_test = filter_new_items(valid_train, valid_test)\n",
    "    return (train, valid_train, valid_test, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### helper function for multiple windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a single item to a window (from 1,...,win) based on timestamp of first item of current session\n",
    "def assign_window(timestamp, cutoff_list):\n",
    "    num_windows = len(cutoff_list)\n",
    "    for i in range(num_windows):\n",
    "        if timestamp <= cutoff_list[i]:\n",
    "            window = i+1\n",
    "            break\n",
    "    return window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apply preprocessing and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_item_support = 5\n",
    "min_session_length = 2\n",
    "min_user_sessions = 3\n",
    "\n",
    "drop_on = True\n",
    "drop_off = True\n",
    "drop_first = False # should always be set to False if drop_on=True\n",
    "\n",
    "multiple_windows = True # flag for multiple windows\n",
    "win = 5 # only needed if multiple_windows=True\n",
    "\n",
    "path = '../../data/preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multiple_windows:\n",
    "    \n",
    "    ts_min = data_sa.timestamp.min()\n",
    "    ts_max = data_sa.timestamp.max()\n",
    "    win_timespan = (ts_max-ts_min)/win\n",
    "    win_cutoffs = [ts_min+(i+1)*win_timespan for i in range(win)]\n",
    "    \n",
    "    # create new column containing timestamp from first item of each session for each item of the session\n",
    "    data_sa['window'] = data_sa['timestamp'].groupby(data_sa['sessionID']).transform('first')\n",
    "\n",
    "    # based on timestamp from first item, assign the entire session to one of the win windows\n",
    "    # to do so, apply assign_window to entire column \"window\"\n",
    "    # this way, we never split up sessions\n",
    "    data_sa['window'] = data_sa['window'].apply(lambda x: assign_window(x, win_cutoffs))\n",
    "    \n",
    "    for i in range(win):\n",
    "        name = 'events' + '-' + str(i+1) # set up dataset name, e.g., data_sa_1 corresponding to windows 1\n",
    "        df = data_sa[data_sa.window==i+1].drop('window',axis=1) # choose one single window only\n",
    "        train, valid_train, valid_test, test = split_data(df,\n",
    "                                                         min_item_support, min_session_length, min_user_sessions,\n",
    "                                                         USER_KEY, ITEM_KEY, TIME_KEY, SESSION_KEY,\n",
    "                                                         drop_on=drop_on, drop_off=drop_off, drop_first=drop_first)\n",
    "        # save output to hdf files\n",
    "        filename = path + str(name)\n",
    "        if drop_on:\n",
    "            filename += '-drop_on'\n",
    "        if drop_off:\n",
    "            filename += '-drop_off'\n",
    "        if drop_first:\n",
    "            filename += '-drop_first'\n",
    "        filename += '.hdf'\n",
    "        \n",
    "        train.to_hdf(filename, key='train', mode='w') # create new file (to avoid adding to existing file)\n",
    "        valid_test.to_hdf(filename, key='valid_test', mode='a')\n",
    "        valid_train.to_hdf(filename, key='valid_train', mode='a')\n",
    "        test.to_hdf(filename, key='test', mode='a')\n",
    "        \n",
    "else:\n",
    "    train, valid_train, valid_test, test = split_data(data_sa,\n",
    "                                                      min_item_support, min_session_length, min_user_sessions,\n",
    "                                                      USER_KEY, ITEM_KEY, TIME_KEY, SESSION_KEY,\n",
    "                                                      drop_on=drop_on, drop_off=drop_off, drop_first=drop_first)\n",
    "    filename = path + 'events'\n",
    "    if drop_on:\n",
    "        filename += '-drop_on'\n",
    "    if drop_off:\n",
    "        filename += '-drop_off'\n",
    "    if drop_first:\n",
    "        filename += '-drop_first'\n",
    "    filename += '.hdf'\n",
    "    \n",
    "    test.to_hdf(filename, key='test', mode='w') # create new file (to avoid adding to existing file)\n",
    "    train.to_hdf(filename, key='train', mode='a')\n",
    "    valid_test.to_hdf(filename, key='valid_test', mode='a')\n",
    "    valid_train.to_hdf(filename, key='valid_train', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change column names to align with retailrocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'events-1-drop_on-drop_off'\n",
    "path = '../../data/preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = path + filename + '.hdf'\n",
    "\n",
    "for split in ['test', 'train', 'valid_test', 'valid_train']:\n",
    "    dataset = pd.read_hdf(filename, split)\n",
    "    dataset = dataset.rename(columns={\"userID\":\"visitorid\", \"appID\":\"itemid\", \"sessionID\":\"session_id\"})\n",
    "    if split == 'test':\n",
    "        mode = 'w' # create new file for 'test', which is the first split (to avoid adding to existing file)\n",
    "    else: mode = 'a' # append all other splits\n",
    "    dataset.to_hdf('../../session-aware_RC_2020/code/data/retailrocket/prepared/events.hdf', key=split, mode=mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
