{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/sequence-level/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pyreadr.read_r('../../data/sequences_all_anon.Rds')[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'datum':'date', 'value':'category_name', 'anon_apps.name':'app_name'}, inplace=True)\n",
    "data['timestamp'] = data['date'].apply(lambda x: x.timestamp())\n",
    "data['category_name'].replace(['OFF_LOCKED', 'OFF_UNLOCKED'], 'OFF', inplace=True)\n",
    "data['category_name'].replace(['ON_LOCKED', 'ON_UNLOCKED'], 'ON', inplace=True)\n",
    "data['sessionID'] = data['category_name'].shift(1).isin(['OFF']).cumsum() + 1 # sessionID is like sequence_number but does NOT start anew for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_mapping = dict([(y,x+1) for x,y in enumerate(sorted(set(data['category_name'])))])\n",
    "# cat_indexes = [cat_mapping[x] for x in data['category_name']]\n",
    "\n",
    "user_mapping = dict([(y,x+1) for x,y in enumerate(sorted(set(data['userId'])))])\n",
    "user_indexes = [user_mapping[x] for x in data['userId']]\n",
    "\n",
    "# data['category'] = cat_indexes\n",
    "data.insert(0, 'userID', user_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = data.groupby(['sessionID'])['category_name'].apply(','.join).reset_index()\n",
    "# a.category_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine consecutive events of same category into single event (by dropping all but the first event)\n",
    "data = data[data.category_name != data.category_name.shift(+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns containing event categories listed (string separated by commas) and as a set\n",
    "data['category_list'] = data.groupby(['sessionID'])['category_name'].transform(','.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_drop_onoff = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "if seq_drop_onoff:\n",
    "    \n",
    "    # first drop all 'ON,OFF' sessions\n",
    "    mask = data['category_list'] == 'ON,OFF'\n",
    "    data = data[~mask]\n",
    "    \n",
    "    # then drop all 'ON' and 'OFF' items from remaining sessions\n",
    "    def remove_from_string(string, to_remove):\n",
    "        l = string.split(',')\n",
    "        out = \",\".join(list(filter(lambda x: (x not in to_remove), l)))\n",
    "        return out\n",
    "    \n",
    "    data['category_list'] = data['category_list'].apply(lambda x: remove_from_string(x, ['ON', 'OFF']))\n",
    "    \n",
    "    filename = 'data_seq_drop_onoff'\n",
    "\n",
    "else:\n",
    "    \n",
    "    filename = 'data_seq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns containing event counts per sequence\n",
    "data['seq_length'] = data['category_list'].apply(lambda x: len(x.split(','))) # all events in sequence\n",
    "# add frequency of category lists and sets\n",
    "data['seq_freq'] = data.groupby(['category_list'])['category_list'].transform(lambda x: x.count())/data['seq_length']\n",
    "# add sequence duration\n",
    "data['seq_duration'] = data.groupby(['sessionID'])['timestamp'].transform('last') - data.groupby(['sessionID'])['timestamp'].transform('first')\n",
    "\n",
    "# data['category_set'] = data['category_list'].apply(lambda x: set(x.split(',')))\n",
    "# data['category_set_count'] = data['category_set'].apply(lambda x: len(x)) # distinct events in sequence\n",
    "# data['category_set_freq'] = data.groupby(['category_set'])['category_set'].transform(lambda x: str(x).count())/data['category_set_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique sequence ID (usID) in order to \"tokenize\" unique sequences\n",
    "seq_mapping = dict([(y,x+1) for x,y in enumerate(sorted(set(data['category_list'])))])\n",
    "seq_indexes = [seq_mapping[x] for x in data['category_list']]\n",
    "\n",
    "data['usID'] = seq_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_mapping['ON,OFF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(path + filename + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'data_seq'\n",
    "filename = 'data_seq'\n",
    "data = pd.read_csv(path + filename + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### by sequence frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for \"min seq freq\" cutoff 1: 99660 unique sequences, a total of 3844150 events, and a total of 844296 sessions\n",
      "for \"min seq freq\" cutoff 2: 14167 unique sequences, a total of 2658851 events, and a total of 758803 sessions\n",
      "for \"min seq freq\" cutoff 5: 4765 unique sequences, a total of 2480048 events, and a total of 735236 sessions\n",
      "for \"min seq freq\" cutoff 10: 2454 unique sequences, a total of 2377540 events, and a total of 720379 sessions\n",
      "for \"min seq freq\" cutoff 20: 1356 unique sequences, a total of 2281361 events, and a total of 705834 sessions\n",
      "for \"min seq freq\" cutoff 50: 613 unique sequences, a total of 2144364 events, and a total of 683557 sessions\n",
      "for \"min seq freq\" cutoff 100: 335 unique sequences, a total of 2033415 events, and a total of 664177 sessions\n"
     ]
    }
   ],
   "source": [
    "for N in [1,2,5,10,20,50,100]:\n",
    "    unique_items = data['category_list'][data['seq_freq']>=N].nunique()\n",
    "    n = data['category_list'][data['seq_freq']>=N].shape[0]\n",
    "    n_sessions = data[data['seq_freq']>=N].groupby(['sessionID']).count().shape[0]\n",
    "    print('for \"min seq freq\" cutoff ' + str(N) + ': ' + str(unique_items) + ' unique sequences, a total of ' + str(n) + ' events, and a total of ' + str(n_sessions) + ' sessions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for N in [10,20,50,100]:\n",
    "#     unique_items = data['category_set'][data['category_set_freq']>=N].nunique()\n",
    "#     n = data['category_set'][data['category_set_freq']>=N].shape[0]\n",
    "#     n_sessions = data[data['category_set_freq']>=N].groupby('sessionID').count().shape[0]\n",
    "#     print('for set cutoff ' + str(N) + ': ' + str(unique_items) + ' unique sets, a total of ' + str(n) + ' events, and a total of ' + str(n_sessions) + ' sessions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### by sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for \"max seq length\" cutoff 1: 0 unique sequences, a total of 0 events, and a total of 0 sessions\n",
      "for \"max seq length\" cutoff 2: 1 unique sequences, a total of 735670 events, and a total of 367835 sessions\n",
      "for \"max seq length\" cutoff 5: 5539 unique sequences, a total of 1796732 events, and a total of 636277 sessions\n",
      "for \"max seq length\" cutoff 10: 54250 unique sequences, a total of 2942495 events, and a total of 796382 sessions\n",
      "for \"max seq length\" cutoff 20: 88936 unique sequences, a total of 3456577 events, and a total of 833515 sessions\n",
      "for \"max seq length\" cutoff 50: 98328 unique sequences, a total of 3728124 events, and a total of 842961 sessions\n",
      "for \"max seq length\" cutoff 100: 99423 unique sequences, a total of 3799901 events, and a total of 844059 sessions\n"
     ]
    }
   ],
   "source": [
    "for N in [1,2,5,10,20,50,100]:\n",
    "    unique_items = data['category_list'][data['seq_length']<=N].nunique()\n",
    "    n = data['category_list'][data['seq_length']<=N].shape[0]\n",
    "    n_sessions = data[data['seq_length']<=N].groupby(['sessionID']).count().shape[0]\n",
    "    print('for \"max seq length\" cutoff ' + str(N) + ': ' + str(unique_items) + ' unique sequences, a total of ' + str(n) + ' events, and a total of ' + str(n_sessions) + ' sessions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### by sequence duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for \"max seq duration\" cutoff 600 seconds: 74614 unique sequences, a total of 3258987 events, and a total of 806750 sessions\n",
      "for \"max seq duration\" cutoff 1200 seconds: 86401 unique sequences, a total of 3496106 events, and a total of 827159 sessions\n",
      "for \"max seq duration\" cutoff 1800 seconds: 91313 unique sequences, a total of 3606515 events, and a total of 834046 sessions\n",
      "for \"max seq duration\" cutoff 2700 seconds: 94603 unique sequences, a total of 3689478 events, and a total of 838252 sessions\n",
      "for \"max seq duration\" cutoff 3600 seconds: 96200 unique sequences, a total of 3732040 events, and a total of 840228 sessions\n"
     ]
    }
   ],
   "source": [
    "for N in [600,1200,1800,2700,3600]:\n",
    "    unique_items = data['category_list'][data['seq_duration']<=N].nunique()\n",
    "    n = data['category_list'][data['seq_duration']<=N].shape[0]\n",
    "    n_sessions = data[data['seq_duration']<=N].groupby(['sessionID']).count().shape[0]\n",
    "    print('for \"max seq duration\" cutoff ' + str(N) + ' seconds: ' + str(unique_items) + ' unique sequences, a total of ' + str(n) + ' events, and a total of ' + str(n_sessions) + ' sessions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### by a combination of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 10\n",
    "N2 = 10\n",
    "N3 = 2700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = data['seq_freq']>=N1\n",
    "mask2 = data['seq_length']<=N2\n",
    "mask3 = data['seq_duration']<=N3\n",
    "mask = mask1 & mask2 & mask3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_items = data['category_list'][mask].nunique()\n",
    "n = data['category_list'][mask].shape[0]\n",
    "n_sessions = data[mask].groupby(['sessionID']).count().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for \"combined\" cutoff: 2408 unique sequences, a total of 2365047 events, and a total of 718937 sessions\n"
     ]
    }
   ],
   "source": [
    "print('for \"combined\" cutoff: '  + str(unique_items) + ' unique sequences, a total of ' + str(n) + ' events, and a total of ' + str(n_sessions) + ' sessions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all sequences with global frequency < 10\n",
    "N = 10\n",
    "# keep only the first row of each sequence\n",
    "data = data[data['seq_freq']>=N].groupby(['sessionID']).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_day = pd.to_datetime(data.timestamp.min(), unit='s').date()\n",
    "end_day = pd.to_datetime(data.timestamp.max(), unit='s').date()\n",
    "day_range = pd.date_range(start_day, end_day, freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper list (same length as data) containing the day\n",
    "user_day = data['userID'].astype(str).str.zfill(3) + '_' + pd.to_datetime(data['timestamp'], unit='s').apply(lambda x: x.date()).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_mapping = dict([(y,x+1) for x,y in enumerate(sorted(set(user_day)))])\n",
    "sentence_indexes = [sentence_mapping[x] for x in user_day]\n",
    "\n",
    "# data['category'] = cat_indexes\n",
    "data['sentenceID'] = sentence_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dropping irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['userId', 'date', 'activity', 'category_name', 'sequence_number', 'app_name', 'category_list', 'seq_length', 'seq_freq', 'seq_duration'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(path + filename + '_final' + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
